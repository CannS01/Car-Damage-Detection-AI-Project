{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CannS01/Car-Damage-Detection-AI-Project/blob/main/Car_Damage_Detection_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QAR9zUaK1url",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --quiet torch torchvision torchaudio\n",
        "!pip install --quiet matplotlib seaborn\n",
        "!pip install --quiet opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFByPWfk0Z1T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nsLkMetyHlC4"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nBO_FrqPvyTp"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnirtgl1v3ln"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lWPnsFLv_Cw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlclQzGHwZBS"
      },
      "outputs": [],
      "source": [
        "import kaggle\n",
        "dataset = kaggle.api.dataset_download_files('samwash94/comprehensive-car-damage-detection', unzip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rZorLssw4Z2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "damage_map = {'Normal': 0, 'Crushed': 1, 'Breakage': 2}\n",
        "view_map = {'F': 0, 'R': 1}\n",
        "\n",
        "class MultiTaskCarDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for folder in os.listdir(root_dir):\n",
        "            folder_path = os.path.join(root_dir, folder)\n",
        "            if not os.path.isdir(folder_path):\n",
        "                continue\n",
        "\n",
        "            view_letter, damage_type = folder.split('_')\n",
        "            view_label = view_map[view_letter]\n",
        "            damage_label = damage_map[damage_type]\n",
        "\n",
        "            for fname in os.listdir(folder_path):\n",
        "                if fname.endswith('.jpg'):\n",
        "                    img_path = os.path.join(folder_path, fname)\n",
        "                    self.samples.append((img_path, view_label, damage_label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, view_label, damage_label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, torch.tensor(view_label), torch.tensor(damage_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vouwLpXI0KrB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_path = \"/content/dataset\"\n",
        "\n",
        "full_dataset = MultiTaskCarDataset(dataset_path, transform=transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Train set: {len(train_dataset)} gÃ¶rÃ¼ntÃ¼\")\n",
        "print(f\"Validation set: {len(val_dataset)} gÃ¶rÃ¼ntÃ¼\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrIBm5UKBvSH"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og5XWxdUBxJT"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/dataset\"\n",
        "train_dataset = MultiTaskCarDataset(dataset_path, transform=train_transform)\n",
        "val_dataset = MultiTaskCarDataset(dataset_path, transform=val_transform)\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_dataset, _ = random_split(train_dataset, [train_size, val_size])\n",
        "_, val_dataset = random_split(val_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jebtngn0vTN"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MultiTaskResNet50(nn.Module):\n",
        "    def __init__(self, num_damage_classes=3, num_view_classes=2):\n",
        "        super(MultiTaskResNet50, self).__init__()\n",
        "        backbone = models.resnet50(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
        "        in_features = backbone.fc.in_features\n",
        "\n",
        "        self.head_damage = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_damage_classes)\n",
        "        )\n",
        "\n",
        "        self.head_view = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_view_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        out_damage = self.head_damage(features)\n",
        "        out_view = self.head_view(features)\n",
        "        return out_damage, out_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "29tlFA0D0y90"
      },
      "outputs": [],
      "source": [
        "model = MultiTaskResNet50()\n",
        "dummy_input = torch.randn(4, 3, 128, 128)\n",
        "out_damage, out_view = model(dummy_input)\n",
        "\n",
        "print(f\"Damage output: {out_damage.shape}\")\n",
        "print(f\"View output:   {out_view.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPXf-q_EXUL3"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=True):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"  âš ï¸ EarlyStopping Counter: {self.counter} / {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQZgcK7dXb9c"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1pns9Zf07i4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = MultiTaskResNet50().to(device)\n",
        "criterion_damage = nn.CrossEntropyLoss()\n",
        "criterion_view = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, scheduler, criterion_damage, criterion_view, device, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_damage = 0\n",
        "        correct_view = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for images, view_labels, damage_labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            view_labels = view_labels.to(device)\n",
        "            damage_labels = damage_labels.to(device)\n",
        "\n",
        "            out_damage, out_view = model(images)\n",
        "\n",
        "            loss_damage = criterion_damage(out_damage, damage_labels)\n",
        "            loss_view = criterion_view(out_view, view_labels)\n",
        "            loss = loss_damage + loss_view\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, pred_damage = torch.max(out_damage, 1)\n",
        "            _, pred_view = torch.max(out_view, 1)\n",
        "\n",
        "            correct_damage += (pred_damage == damage_labels).sum().item()\n",
        "            correct_view += (pred_view == view_labels).sum().item()\n",
        "            total_samples += images.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        acc_damage = 100 * correct_damage / total_samples\n",
        "        acc_view = 100 * correct_view / total_samples\n",
        "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} | Damage Acc: {acc_damage:.2f}% | View Acc: {acc_view:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct_damage = 0\n",
        "        val_correct_view = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, view_labels, damage_labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                view_labels = view_labels.to(device)\n",
        "                damage_labels = damage_labels.to(device)\n",
        "\n",
        "                out_damage, out_view = model(images)\n",
        "\n",
        "                loss_damage = criterion_damage(out_damage, damage_labels)\n",
        "                loss_view = criterion_view(out_view, view_labels)\n",
        "                loss = loss_damage + loss_view\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, pred_damage = torch.max(out_damage, 1)\n",
        "                _, pred_view = torch.max(out_view, 1)\n",
        "\n",
        "                val_correct_damage += (pred_damage == damage_labels).sum().item()\n",
        "                val_correct_view += (pred_view == view_labels).sum().item()\n",
        "                val_total += images.size(0)\n",
        "\n",
        "        val_avg_loss = val_loss / len(val_loader)\n",
        "        val_acc_damage = 100 * val_correct_damage / val_total\n",
        "        val_acc_view = 100 * val_correct_view / val_total\n",
        "\n",
        "        print(f\"               â†’ Val Loss: {val_avg_loss:.4f} | Damage Acc: {val_acc_damage:.2f}% | View Acc: {val_acc_view:.2f}%\")\n",
        "\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J6OMgT8m6QYz"
      },
      "outputs": [],
      "source": [
        "train(model, train_loader, val_loader, optimizer, scheduler,\n",
        "      criterion_damage, criterion_view, device, num_epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WHl25JpoHBq"
      },
      "outputs": [],
      "source": [
        "!pip install -q scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qh3Y4TbqoKlp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds_damage = []\n",
        "    all_labels_damage = []\n",
        "    all_preds_view = []\n",
        "    all_labels_view = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, view_labels, damage_labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            view_labels = view_labels.to(device)\n",
        "            damage_labels = damage_labels.to(device)\n",
        "\n",
        "            out_damage, out_view = model(images)\n",
        "\n",
        "            _, pred_damage = torch.max(out_damage, 1)\n",
        "            _, pred_view = torch.max(out_view, 1)\n",
        "\n",
        "            all_preds_damage.extend(pred_damage.cpu().numpy())\n",
        "            all_labels_damage.extend(damage_labels.cpu().numpy())\n",
        "\n",
        "            all_preds_view.extend(pred_view.cpu().numpy())\n",
        "            all_labels_view.extend(view_labels.cpu().numpy())\n",
        "\n",
        "    print(\"ðŸ”§ Classification Report: DAMAGE (Normal / Crushed / Breakage)\")\n",
        "    print(classification_report(\n",
        "        all_labels_damage,\n",
        "        all_preds_damage,\n",
        "        target_names=[\"Normal\", \"Crushed\", \"Breakage\"]\n",
        "    ))\n",
        "\n",
        "    print(\"ðŸ”§ Classification Report: VIEW (Front / Rear)\")\n",
        "    print(classification_report(\n",
        "        all_labels_view,\n",
        "        all_preds_view,\n",
        "        target_names=[\"Front\", \"Rear\"]\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6Pa7m6euoPIW"
      },
      "outputs": [],
      "source": [
        "evaluate_model(model, val_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lb-qM8K_cTG"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/multitask_resnet50.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roeFO6lsBy2S"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MultiTaskDenseNet121(nn.Module):\n",
        "    def __init__(self, num_damage_classes=3, num_view_classes=2):\n",
        "        super(MultiTaskDenseNet121, self).__init__()\n",
        "        base_model = models.densenet121(pretrained=True)\n",
        "        self.features = base_model.features\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        in_features = 1024\n",
        "\n",
        "        self.head_damage = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_damage_classes)\n",
        "        )\n",
        "\n",
        "        self.head_view = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_view_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.pool(x)\n",
        "        out_damage = self.head_damage(x)\n",
        "        out_view = self.head_view(x)\n",
        "        return out_damage, out_view\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dkvbVT1AB2kT"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultiTaskDenseNet121().to(device)\n",
        "\n",
        "criterion_damage = nn.CrossEntropyLoss()\n",
        "criterion_view = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6kk2w8O8Ml3"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=True):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"  âš ï¸ EarlyStopping Counter: {self.counter} / {self.patience}\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDRvbedW8Toy"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWOJ0rGvB4du"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, scheduler, criterion_damage, criterion_view, device, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_damage = 0\n",
        "        correct_view = 0\n",
        "        total_samples = 0\n",
        "\n",
        "        for images, view_labels, damage_labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            view_labels = view_labels.to(device)\n",
        "            damage_labels = damage_labels.to(device)\n",
        "\n",
        "            out_damage, out_view = model(images)\n",
        "            loss_damage = criterion_damage(out_damage, damage_labels)\n",
        "            loss_view = criterion_view(out_view, view_labels)\n",
        "            loss = loss_damage + loss_view\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, pred_damage = torch.max(out_damage, 1)\n",
        "            _, pred_view = torch.max(out_view, 1)\n",
        "\n",
        "            correct_damage += (pred_damage == damage_labels).sum().item()\n",
        "            correct_view += (pred_view == view_labels).sum().item()\n",
        "            total_samples += images.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        acc_damage = 100 * correct_damage / total_samples\n",
        "        acc_view = 100 * correct_view / total_samples\n",
        "        print(f\"[Epoch {epoch+1}/{num_epochs}] Train Loss: {avg_loss:.4f} | Damage Acc: {acc_damage:.2f}% | View Acc: {acc_view:.2f}%\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct_damage = 0\n",
        "        val_correct_view = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, view_labels, damage_labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                view_labels = view_labels.to(device)\n",
        "                damage_labels = damage_labels.to(device)\n",
        "\n",
        "                out_damage, out_view = model(images)\n",
        "                loss_damage = criterion_damage(out_damage, damage_labels)\n",
        "                loss_view = criterion_view(out_view, view_labels)\n",
        "                loss = loss_damage + loss_view\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, pred_damage = torch.max(out_damage, 1)\n",
        "                _, pred_view = torch.max(out_view, 1)\n",
        "\n",
        "                val_correct_damage += (pred_damage == damage_labels).sum().item()\n",
        "                val_correct_view += (pred_view == view_labels).sum().item()\n",
        "                val_total += images.size(0)\n",
        "\n",
        "        val_avg_loss = val_loss / len(val_loader)\n",
        "        val_acc_damage = 100 * val_correct_damage / val_total\n",
        "        val_acc_view = 100 * val_correct_view / val_total\n",
        "        print(f\"               â†’ Val Loss: {val_avg_loss:.4f} | Damage Acc: {val_acc_damage:.2f}% | View Acc: {val_acc_view:.2f}%\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        early_stopping(val_avg_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"â›” Early stopping triggered. Training stopped.\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq5c7XiyB6IQ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "train(model, train_loader, val_loader, optimizer, scheduler,\n",
        "      criterion_damage, criterion_view, device, num_epochs=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQhaoxeYCZXB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_separate_losses(damage_train, damage_val, view_train, view_val):\n",
        "    epochs = range(1, len(damage_train) + 1)\n",
        "\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Damage loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, damage_train, label='Train Damage Loss')\n",
        "    plt.plot(epochs, damage_val, label='Val Damage Loss')\n",
        "    plt.title('Damage Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # View loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, view_train, label='Train View Loss')\n",
        "    plt.plot(epochs, view_val, label='Val View Loss')\n",
        "    plt.title('View Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYwe3ucHBBlx"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/densenet121.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1wfuruBBHEA"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/densenet121.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JI0TIquC646"
      },
      "outputs": [],
      "source": [
        "!pip install -q scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evi7DUYqC9A9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "\n",
        "def evaluate_classification_report(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds_damage = []\n",
        "    all_labels_damage = []\n",
        "    all_preds_view = []\n",
        "    all_labels_view = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, view_labels, damage_labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            view_labels = view_labels.to(device)\n",
        "            damage_labels = damage_labels.to(device)\n",
        "\n",
        "            out_damage, out_view = model(images)\n",
        "\n",
        "            _, preds_damage = torch.max(out_damage, 1)\n",
        "            _, preds_view = torch.max(out_view, 1)\n",
        "\n",
        "            all_preds_damage.extend(preds_damage.cpu().numpy())\n",
        "            all_labels_damage.extend(damage_labels.cpu().numpy())\n",
        "\n",
        "            all_preds_view.extend(preds_view.cpu().numpy())\n",
        "            all_labels_view.extend(view_labels.cpu().numpy())\n",
        "\n",
        "    print(\"Classification Report: DAMAGE (Normal / Crushed / Breakage)\")\n",
        "    print(classification_report(all_labels_damage, all_preds_damage, target_names=[\"Normal\", \"Crushed\", \"Breakage\"]))\n",
        "\n",
        "    print(\"Classification Report: VIEW (Front / Rear)\")\n",
        "    print(classification_report(all_labels_view, all_preds_view, target_names=[\"Front\", \"Rear\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6OPvYGAlC-uC"
      },
      "outputs": [],
      "source": [
        "evaluate_classification_report(model, val_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyhIaNHkIVG0"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"multitask_densenet121.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWYnSB2nK4bK"
      },
      "outputs": [],
      "source": [
        "torch.save(model, \"full_model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9b8PhJLZJRX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gZLFpYAZMyH"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrices(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_preds_damage = []\n",
        "    all_labels_damage = []\n",
        "    all_preds_view = []\n",
        "    all_labels_view = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, view_labels, damage_labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            view_labels = view_labels.to(device)\n",
        "            damage_labels = damage_labels.to(device)\n",
        "\n",
        "            out_damage, out_view = model(images)\n",
        "\n",
        "            _, preds_damage = torch.max(out_damage, 1)\n",
        "            _, preds_view = torch.max(out_view, 1)\n",
        "\n",
        "            all_preds_damage.extend(preds_damage.cpu().numpy())\n",
        "            all_labels_damage.extend(damage_labels.cpu().numpy())\n",
        "\n",
        "            all_preds_view.extend(preds_view.cpu().numpy())\n",
        "            all_labels_view.extend(view_labels.cpu().numpy())\n",
        "\n",
        "    # DAMAGE MATRIX\n",
        "    cm_damage = confusion_matrix(all_labels_damage, all_preds_damage)\n",
        "    labels_damage = [\"Normal\", \"Crushed\", \"Breakage\"]\n",
        "\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(cm_damage, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=labels_damage, yticklabels=labels_damage)\n",
        "    plt.title(\"Confusion Matrix - Damage\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "    # VIEW MATRIX\n",
        "    cm_view = confusion_matrix(all_labels_view, all_preds_view)\n",
        "    labels_view = [\"Front\", \"Rear\"]\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    sns.heatmap(cm_view, annot=True, fmt=\"d\", cmap=\"Greens\",\n",
        "                xticklabels=labels_view, yticklabels=labels_view)\n",
        "    plt.title(\"Confusion Matrix - View\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SdSNeKPWZPgK"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrices(model, val_loader, device)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPcigMELnWjiitAfoPvF1lN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}